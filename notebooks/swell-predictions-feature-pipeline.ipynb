{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6d89da",
   "metadata": {},
   "source": [
    "## Feature Pipeline for the `exploded_swells` feature group\n",
    "\n",
    "This feature pipeline can be run on a schedule using github actions (see github repo for the example file).\n",
    "\n",
    "### Requirements\n",
    "\n",
    " * pip install hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdbcfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request  \n",
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hopsworks\n",
    "from datetime import datetime, timedelta\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46146619",
   "metadata": {},
   "source": [
    "### Not app.hopsworks.ai ?\n",
    "\n",
    "If you are running your own Hopsworks cluster (not app.hopsworks.ai):\n",
    "\n",
    " * uncomment the cell below\n",
    " * fill in details for your cluster\n",
    " * run the cel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key=\"\"\n",
    "#with open(\"api-key.txt\", \"r\") as f:\n",
    "#    key = f.read().rstrip()\n",
    "#os.environ['HOPSWORKS_PROJECT']=\"cjsurf\"\n",
    "#os.environ['HOPSWORKS_HOST']=\"35.187.178.84\"\n",
    "#os.environ['HOPSWORKS_API_KEY']=key    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c163f",
   "metadata": {},
   "source": [
    "### Backfill the feature group \n",
    "\n",
    "If you set `BACKFILL` to `True` in the cell below, and continue running all the cells, you will insert swell predictions from the `swells-clean.csv` file into the feature group.\n",
    "\n",
    "When `BACKFILL` is `False`, it will download the latest predictions from the NOA 62081 Buoy and insert them into the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKFILL=False\n",
    "if os.environ.get('BACKFILL') == \"False\":\n",
    "    BACKFILL=False\n",
    "hours=119\n",
    "version=1\n",
    "backfill_url=\"https://repo.hops.works/master/hopsworks-tutorials/data/cjsurf/swells-clean.csv\"\n",
    "buoy=\"62081\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_url(today):\n",
    "    pred_date = today.strftime(\"%Y%m%d\")\n",
    "\n",
    "    # There are 4 predictions per day at hours: \"00\", \"06\", \"12\", \"18\",\n",
    "    h=int(today.strftime(\"%H\"))\n",
    "    found = False\n",
    "    test_url = \"\"\n",
    "    attempted_date = today\n",
    "\n",
    "    while not found:\n",
    "        pred_hour = \"00\"\n",
    "        if h > 5:\n",
    "            pred_hour = \"06\"\n",
    "        if h > 11:\n",
    "            pred_hour = \"12\" \n",
    "        if h > 17:\n",
    "            pred_hour = \"18\"\n",
    "        test_url = \"https://ftpprd.ncep.noaa.gov/data/nccf/com/gfs/prod/gfs.\" \\\n",
    "        + attempted_date.strftime(\"%Y%m%d\") + \\\n",
    "        \"/\" + pred_hour + \"/wave/station/bulls.t\" + pred_hour + \"z/gfswave.\" + buoy + \".bull\"\n",
    "        try:\n",
    "            urllib.request.urlopen(test_url)\n",
    "            found = True\n",
    "        except urllib.error.HTTPError as e: \n",
    "            # assume 404, URL not found. Try previous time.\n",
    "            h = h - 6\n",
    "            if h < 0:\n",
    "                attempted_date = attempted_date - timedelta(days=1)\n",
    "                # if i have to look back >1 day, then just exit with error - because upstream is prob broken\n",
    "                if (today.day - attempted_date.day > 1):\n",
    "                    sys.exit(\"ERROR: Could not download url: \" + test_url) \n",
    "    print(test_url)\n",
    "    return test_url, pred_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9c9da",
   "metadata": {},
   "source": [
    "### Understand the Features\n",
    "\n",
    "We store 119*4=476 columns in the `swell_predictions` feature group. It is 119 different swell predictions, one for each hour from hour=0, hour=2, ..., hour=238.  Each prediction is made using the `height`, `period`, and `direction` features. The `hits_at` feature is used to estimate the time at which the swell arrives at Lahinch beach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc73480",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_columns=[]\n",
    "for i in range(1,hours):\n",
    "    j=i*2\n",
    "    secondary_columns.append(\"height\" + str(j))\n",
    "    secondary_columns.append(\"period\" + str(j))\n",
    "    secondary_columns.append(\"direction\" + str(j))\n",
    "    secondary_columns.append(\"hits_at\" + str(j))\n",
    "\n",
    "secondary_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751537c6",
   "metadata": {},
   "source": [
    "Parse the data in the URL managed by NOA containing the predictions for the Buoy:\n",
    "\n",
    "https://ftpprd.ncep.noaa.gov/data/nccf/com/gfs/prod/gfs.DATE/HOUR/wave/station/bulls.tHOURz/gfswave.62081.bull \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(buoy_url):\n",
    "    out = []\n",
    "    for line in urllib.request.urlopen(buoy_url):\n",
    "        l = line.decode('utf-8') #utf-8 or iso8859-1 or whatever the page encoding scheme is\n",
    "        row=[]\n",
    "        if \"Cycle\" in l:\n",
    "            regex = re.findall(r'Cycle.*:\\s+([0-9]+)\\s+([0-9]+)\\s+UTC.*', l)\n",
    "            if len(regex):\n",
    "                thedate=regex[0]\n",
    "        else:\n",
    "            res = re.match(r'.*[|]\\s+([0-9]+)\\s+([0-9]+)\\s+[|].*', l)\n",
    "            waves = re.findall(r'[|]\\s+([0-9\\.]+)\\s+([0-9\\.]+)\\s+([0-9]+)\\s+[|]', l)\n",
    "            if res is not None:\n",
    "                row.append(thedate)\n",
    "                row.append(res.groups())\n",
    "            if len(waves):\n",
    "                if len(waves) > 3:\n",
    "                    # print(\"found > 3 waves, reduce to 3\")\n",
    "                    waves = waves[:3]\n",
    "                b = []\n",
    "                list(b.extend(item) for item in waves)\n",
    "                row.append(b)\n",
    "                my = tuple(chain.from_iterable(row))\n",
    "                out.append(my)\n",
    "    return out, thedate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff4b79",
   "metadata": {},
   "source": [
    "### Feature engineering - select the best swell for Lahinch\n",
    "\n",
    "There are between zero and 6 different swells. Extract the swell that is gives the expected highest surf at Lahinch, based on the angle of the swell direction (Lahinch has a swell direction window of around 20 degrees to 120 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c54672",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_columns=['pred_dtime', 'hour', 'pred_day', 'pred_hour', 'height1', 'period1', 'direction1', 'height2', \n",
    "         'period2', 'direction2', 'height3', 'period3', 'direction3'] \n",
    "\n",
    "def is_valid_swell_direction(direction):\n",
    "    if int(direction) > 180 or int(direction) < 20:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def best_height(row):\n",
    "    best_secondary=2\n",
    "    # Check which is best secondary swell - swell 2 or swell 3?\n",
    "    if row['direction3'] != None:\n",
    "        if is_valid_swell_direction(row['direction3']):\n",
    "            if is_valid_swell_direction(row['direction2']) == False :\n",
    "                best_secondary=3    \n",
    "    best_direction = \"direction\" + str(best_secondary)\n",
    "    best=1\n",
    "    # Check which is best of swell 1 and secondary swell ?\n",
    "    if row[best_direction] != None and is_valid_swell_direction(row[best_direction]) == True:\n",
    "        if is_valid_swell_direction(row['direction1']) == False:\n",
    "            best=best_secondary\n",
    "                \n",
    "    height = row['height' + str(best)]\n",
    "    period = row['period' + str(best)]\n",
    "    direction = row['direction' + str(best)]\n",
    "        \n",
    "    return pd.Series([height, period, direction])\n",
    "\n",
    "# feature engineering - estimate the time at which the swell arrives at Lahinch from buoy\n",
    "def estimate_hits_at(row):\n",
    "    # baseline estimate\n",
    "    hits_at = row['pred_dtime'] + row['hour_offset'] + timedelta(hours=8) \n",
    "    \n",
    "    if float(row['direction']) < 80 and float(row['direction']) > 66:\n",
    "        hits_at = hits_at - timedelta(hours=1)\n",
    "    if float(row['direction']) <= 66 and float(row['direction']) > 50:\n",
    "        hits_at = hits_at - timedelta(hours=2)\n",
    "    if float(row['direction']) <= 50 and float(row['direction']) > 20:\n",
    "        hits_at = hits_at - timedelta(hours=3)\n",
    "    if float(row['period']) > 12:\n",
    "        hits_at = hits_at - timedelta(hours=1)\n",
    "    \n",
    "    return pd.Series([hits_at])\n",
    "    \n",
    "\n",
    "if BACKFILL == True:\n",
    "    df = pd.read_csv(backfill_url, parse_dates=['hits_at', 'pred_dtime'])\n",
    "    num_rows = df.shape[0]\n",
    "    print(\"num_rows: \" + str(num_rows))\n",
    "    rows = []\n",
    "    for i in range(1, num_rows):\n",
    "        row=[]\n",
    "        for j in range(0, len(secondary_columns)):\n",
    "            row.append(\"\")\n",
    "        if i % 2 == 0:\n",
    "            rows.append(row)\n",
    "    df_secondary = pd.DataFrame(rows, columns=secondary_columns)\n",
    "    df = pd.concat([df, df_secondary],axis=1, join=\"outer\")    \n",
    "    \n",
    "else: # BACKFILL == False\n",
    "    today = datetime.now()\n",
    "    url, pred_hour = get_latest_url(today)\n",
    "    print(url)\n",
    "    res,thedate=process_url(url)\n",
    "    df = pd.DataFrame(res, columns=primary_columns)\n",
    "    df['pred_dtime'] = pd.to_datetime(df['pred_dtime'], format='%Y%m%d')\n",
    "    df.insert(loc=0, column=\"hour_offset\", value=(df.reset_index().index*2))\n",
    "    df['hour_offset'] = df.hour_offset.astype('timedelta64[h]')\n",
    "    df['pred_dtime'] = df['pred_dtime'] + df.hour.astype('timedelta64[h]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7636c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BACKFILL == False:\n",
    "    df[['height','period','direction']]=df.apply(best_height, axis=1)\n",
    "    df[['hits_at']]=df.apply(estimate_hits_at, axis=1)\n",
    "    df['beach_id'] = 1\n",
    "    df.drop(['height1', 'period1', 'direction1', 'height2', 'period2', 'direction2', 'hour_offset',\n",
    "              'height3', 'period3', 'direction3','hour', 'pred_day', 'pred_hour'], axis=1, inplace=True) \n",
    "\n",
    "df['height'] = pd.to_numeric(df['height'] , errors='coerce').astype(np.float64)\n",
    "df['period'] = pd.to_numeric(df['period'] , errors='coerce').astype(np.float64)\n",
    "df['direction'] = pd.to_numeric(df['direction'] , errors='coerce').astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3490c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [\"height\", \"period\", \"direction\", \"hits_at\"]\n",
    "\n",
    "if BACKFILL == False:\n",
    "    entry = []\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        if (index==0):\n",
    "            data.append(row['beach_id'])\n",
    "            data.append(row['pred_dtime'])\n",
    "        if (index < hours):\n",
    "            for m in matches:\n",
    "                data.append(row[m])\n",
    "\n",
    "    entry.append(data)\n",
    "    first_columns=['beach_id', 'pred_dtime', 'height', 'period', 'direction', 'hits_at']    \n",
    "    all_columns = first_columns + secondary_columns\n",
    "    df2 = pd.DataFrame(entry, columns=all_columns)\n",
    "else:    \n",
    "    df2=df\n",
    "\n",
    "for i in range(1,hours):\n",
    "    for j in matches:\n",
    "      df2[j+str(i*2)] = pd.to_numeric(df2[j+str(i*2)]).astype(np.float64)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b00b8a",
   "metadata": {},
   "source": [
    "### Connect to your Hopsworks cluster\n",
    "\n",
    "If you only set the HOPSWORKS_API_KEY, it will assume you are connecting to app.hopsworks.ai.\n",
    "Set HOPSWORKS_HOST and HOPSWORKS_PROJECT environment variables to connect to a different Hopsworks cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ea0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4686d8",
   "metadata": {},
   "source": [
    "Write your features to the `swells_exploded` feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc6ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "swells_fg = fs.get_or_create_feature_group(name=\"swells_exploded\",\n",
    "                version=version,\n",
    "                primary_key=[\"beach_id\"],\n",
    "                event_time=\"hits_at\",\n",
    "                description=\"Buoy surf height predictions\",\n",
    "                online_enabled=True,\n",
    "                statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True}\n",
    "                )\n",
    "swells_fg.insert(df2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ed797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "pred_date = datetime.now()\n",
    "\n",
    "days_to_subtract=1\n",
    "attempted_date = pred_date\n",
    "\n",
    "attempted_date = attempted_date - timedelta(days=days_to_subtract)\n",
    "res= pred_date - attempted_date\n",
    "\n",
    "if res.days > 0 :\n",
    "    print(\"yes\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945396e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
