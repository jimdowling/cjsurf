{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6d89da",
   "metadata": {},
   "source": [
    "## Feature Pipeline for the `exploded_swells` feature group\n",
    "\n",
    "This feature pipeline can be run on a schedule using github actions (see github repo for the example file).\n",
    "\n",
    "### Requirements\n",
    "\n",
    " * hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1939145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdbcfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request  \n",
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hopsworks\n",
    "from datetime import timedelta\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46146619",
   "metadata": {},
   "source": [
    "### Not app.hopsworks.ai ?\n",
    "\n",
    "If you are running your own Hopsworks cluster (not app.hopsworks.ai):\n",
    "\n",
    " * uncomment the cell below\n",
    " * fill in details for your cluster\n",
    " * run the cel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ca44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key=\"\"\n",
    "#with open(\"api-key.txt\", \"r\") as f:\n",
    "#    key = f.read().rstrip()\n",
    "#os.environ['HOPSWORKS_PROJECT']=\"dowlingj\"\n",
    "#os.environ['HOPSWORKS_HOST']=\"35.187.178.84\"\n",
    "#os.environ['HOPSWORKS_API_KEY']=key    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c163f",
   "metadata": {},
   "source": [
    "### Backfill the feature group \n",
    "\n",
    "If you set `BACKFILL` to `True` in the cell below, and continue running all the cells, you will insert swell predictions from the `swells-clean.csv` file into the feature group.\n",
    "\n",
    "When `BACKFILL` is `False`, it will download the latest predictions from the NOA 62081 Buoy and insert them into the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fe2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training columns - height, period, direction, hits_at\n",
    "# Prediction columns - pred_dtime', 'hour', 'pred_day', 'pred_hour\n",
    "# event_time column - hits_at\n",
    "# primary key - beach_id\n",
    "\n",
    "BACKFILL=True\n",
    "if os.environ.get('CJSURF_BACKFILL') == \"False\":\n",
    "    BACKFILL=False\n",
    "hours=119\n",
    "version=1\n",
    "buoy = \"62081\"\n",
    "backfill_url=\"https://repo.hops.works/master/hopsworks-tutorials/data/cjsurf/swells-clean.csv\"        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9c9da",
   "metadata": {},
   "source": [
    "### Understand the Features\n",
    "\n",
    "We store 119*4=476 columns in the `swell_predictions` feature group. It is 119 different swell predictions, one for each hour from hour=0, hour=2, ..., hour=238.  Each prediction is made using the `height`, `period`, and `direction` features. The `hits_at` feature is used to estimate the time at which the swell arrives at Lahinch beach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751537c6",
   "metadata": {},
   "source": [
    "Parse the data in the URL managed by NOA containing the predictions for the Buoy:\n",
    "\n",
    "https://polar.ncep.noaa.gov/waves/WEB/gfswave.latest_run/plots/gfswave.62081.bull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7387f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to scrap latest swell predictions\n",
    "# Format for date is 'YYYYMMDD', e.g., 20220817\n",
    "def get_latest_url(today):\n",
    "    pred_date = today.strftime(\"%Y%m%d\")\n",
    "    pred_date = \"20220820\"\n",
    "\n",
    "    # There are 4 predictions per day at hours: \"00\", \"06\", \"12\", \"18\",\n",
    "    h=int(today.strftime(\"%H\"))\n",
    "    found = False\n",
    "    test_url = \"\"\n",
    "    while not found:\n",
    "        pred_hour = \"00\"\n",
    "        if h > 5:\n",
    "            pred_hour = \"06\"\n",
    "        if h > 11:\n",
    "            pred_hour = \"12\" \n",
    "        if h > 17:\n",
    "            pred_hour = \"18\"\n",
    "        test_url = \"https://ftpprd.ncep.noaa.gov/data/nccf/com/gfs/prod/gfs.\" + pred_date + \\\n",
    "      \"/\" + pred_hour + \"/wave/station/bulls.t\" + pred_hour + \"z/gfswave.\" + buoy + \".bull\"\n",
    "        print(\"test_url\")\n",
    "        try:\n",
    "            urllib.request.urlopen(test_url)\n",
    "            found = True\n",
    "        except urllib.error.HTTPError as e: \n",
    "            # assume 404, URL not found. Try previous time.\n",
    "            h = h - 6\n",
    "            if h < 0:\n",
    "                print(\"ERROR: Could not download url: \" + test_url)\n",
    "                exit()\n",
    "    return test_url, pred_hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e410a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(buoy_url):\n",
    "    out = []\n",
    "    c=0\n",
    "    for line in urllib.request.urlopen(buoy_url):\n",
    "        l = line.decode('utf-8') #utf-8 or iso8859-1 or whatever the page encoding scheme is\n",
    "        row=[]\n",
    "        if \"Cycle\" in l:\n",
    "            # Parse this line \"Cycle    : 20220818  6 UTC\"\n",
    "            regex = re.findall(r'Cycle.*:\\s+([0-9]+)\\s+([0-9]+)\\s+UTC.*', l)\n",
    "            if len(regex):\n",
    "                thedate=regex[0]\n",
    "        else:\n",
    "            res = re.match(r'.*[|]\\s+([0-9]+)\\s+([0-9]+)\\s+[|].*', l)\n",
    "            waves = re.findall(r'[|][\\s\\*]+([0-9\\.]+)\\s+([0-9\\.]+)\\s+([0-9]+)\\s+[|]', l)\n",
    "            if res is not None:\n",
    "                row.append(thedate)\n",
    "                row.append(res.groups())\n",
    "            if len(waves):\n",
    "                if len(waves) > 3:\n",
    "                    # print(\"found > 3 waves, reduce to 3\")\n",
    "                    waves = waves[:3]\n",
    "                b = []\n",
    "                list(b.extend(item) for item in waves)\n",
    "                row.append(b)\n",
    "                my = tuple(chain.from_iterable(row))\n",
    "                out.append(my)\n",
    "    return out, thedate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff4b79",
   "metadata": {},
   "source": [
    "### Feature engineering - select the best swell for Lahinch\n",
    "\n",
    "Each hour with a prediction at https://polar.ncep.noaa.gov/waves/WEB/gfswave.latest_run/plots/gfswave.62081.bull can contain zero to many different swells. Extract the swell that is gives the expected highest surf at Lahinch, based on the angle of the swell direction (Lahinch has a swell direction window of around 20 degrees to 120 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0c54672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beach_id</th>\n",
       "      <th>height</th>\n",
       "      <th>period</th>\n",
       "      <th>direction</th>\n",
       "      <th>hits_at</th>\n",
       "      <th>pred_dtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>11.2</td>\n",
       "      <td>87</td>\n",
       "      <td>2004-03-18 08:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>83</td>\n",
       "      <td>2004-03-18 07:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2.7</td>\n",
       "      <td>10.4</td>\n",
       "      <td>80</td>\n",
       "      <td>2004-05-17 06:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>11.2</td>\n",
       "      <td>75</td>\n",
       "      <td>2004-03-18 05:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3.4</td>\n",
       "      <td>11.3</td>\n",
       "      <td>78</td>\n",
       "      <td>2004-03-18 03:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13143</th>\n",
       "      <td>1</td>\n",
       "      <td>1.6</td>\n",
       "      <td>11.3</td>\n",
       "      <td>118</td>\n",
       "      <td>2005-10-18 14:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13144</th>\n",
       "      <td>1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>11.4</td>\n",
       "      <td>118</td>\n",
       "      <td>2005-10-18 16:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13145</th>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>11.4</td>\n",
       "      <td>119</td>\n",
       "      <td>2005-10-18 17:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13146</th>\n",
       "      <td>1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>11.4</td>\n",
       "      <td>119</td>\n",
       "      <td>2005-10-18 18:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13147</th>\n",
       "      <td>1</td>\n",
       "      <td>1.2</td>\n",
       "      <td>11.5</td>\n",
       "      <td>119</td>\n",
       "      <td>2005-10-18 19:00:00</td>\n",
       "      <td>2004-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13148 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       beach_id  height  period  direction             hits_at pred_dtime\n",
       "0             1     3.1    11.2         87 2004-03-18 08:00:00 2004-01-01\n",
       "1             1     3.4    11.2         83 2004-03-18 07:00:00 2004-01-01\n",
       "2             1     2.7    10.4         80 2004-05-17 06:00:00 2004-01-01\n",
       "3             1     3.4    11.2         75 2004-03-18 05:00:00 2004-01-01\n",
       "4             1     3.4    11.3         78 2004-03-18 03:00:00 2004-01-01\n",
       "...         ...     ...     ...        ...                 ...        ...\n",
       "13143         1     1.6    11.3        118 2005-10-18 14:00:00 2004-01-01\n",
       "13144         1     1.4    11.4        118 2005-10-18 16:00:00 2004-01-01\n",
       "13145         1     1.3    11.4        119 2005-10-18 17:00:00 2004-01-01\n",
       "13146         1     1.3    11.4        119 2005-10-18 18:00:00 2004-01-01\n",
       "13147         1     1.2    11.5        119 2005-10-18 19:00:00 2004-01-01\n",
       "\n",
       "[13148 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_columns=['pred_dtime', 'hour', 'pred_day', 'pred_hour', 'height1', 'period1', 'direction1', 'height2', \n",
    "         'period2', 'direction2', 'height3', 'period3', 'direction3'] \n",
    "\n",
    "def is_valid_swell_direction(direction):\n",
    "    if int(direction) > 180 or int(direction) < 20:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def best_height(row):\n",
    "    best_secondary=2\n",
    "    # Check which is best secondary swell - swell 2 or swell 3?\n",
    "    if row['direction3'] != None:\n",
    "        if is_valid_swell_direction(row['direction3']):\n",
    "            if is_valid_swell_direction(row['direction2']) == False :\n",
    "                best_secondary=3    \n",
    "    best_direction = \"direction\" + str(best_secondary)\n",
    "    best=1\n",
    "    # Check which is best of swell 1 and secondary swell ?\n",
    "    if row[best_direction] != None and is_valid_swell_direction(row[best_direction]) == True:\n",
    "        if is_valid_swell_direction(row['direction1']) == False:\n",
    "            best=best_secondary\n",
    "                \n",
    "    height = row['height' + str(best)]\n",
    "    period = row['period' + str(best)]\n",
    "    direction = row['direction' + str(best)]\n",
    "        \n",
    "    return pd.Series([height, period, direction])\n",
    "\n",
    "# feature engineering - estimate the time at which the swell arrives at Lahinch from buoy\n",
    "def estimate_hits_at(row):\n",
    "    # baseline estimate\n",
    "    hits_at = row['pred_dtime'] + row['hour_offset'] + timedelta(hours=8) \n",
    "    \n",
    "    if float(row['direction']) < 80 and float(row['direction']) > 66:\n",
    "        hits_at = hits_at - timedelta(hours=1)\n",
    "    if float(row['direction']) <= 66 and float(row['direction']) > 50:\n",
    "        hits_at = hits_at - timedelta(hours=2)\n",
    "    if float(row['direction']) <= 50 and float(row['direction']) > 20:\n",
    "        hits_at = hits_at - timedelta(hours=3)\n",
    "    if float(row['period']) > 12:\n",
    "        hits_at = hits_at - timedelta(hours=1)\n",
    "    \n",
    "    return pd.Series([hits_at])\n",
    "    \n",
    "\n",
    "if BACKFILL == True:\n",
    "    df = pd.read_csv(backfill_url, parse_dates=['hits_at', 'pred_dtime'])\n",
    "#     num_rows = df.shape[0]\n",
    "#     print(\"num_rows: \" + str(num_rows))\n",
    "#     rows = []\n",
    "#     for i in range(1, num_rows):\n",
    "#         row=[]\n",
    "#         for j in range(0, len(secondary_columns)):\n",
    "#             row.append(\"\")\n",
    "#         if i % 2 == 0:\n",
    "#             rows.append(row)\n",
    "#    df_secondary = pd.DataFrame(rows, columns=secondary_columns)\n",
    "#    df = pd.concat([df, df_secondary],axis=1, join=\"outer\")    \n",
    "    \n",
    "else: # BACKFILL == False\n",
    "    today = datetime.now()\n",
    "    url, pred_hour = get_latest_url(today)\n",
    "    print(int(pred_hour))\n",
    "    res,thedate=process_url(url)\n",
    "    df = pd.DataFrame(res, columns=scraped_columns)\n",
    "    df['pred_dtime'] = pd.to_datetime(df['pred_dtime'], format='%Y%m%d')\n",
    "#    df['pred_dtime']  = df['pred_dtime']  + timedelta(hours=int(pred_hour)) \n",
    "    df.insert(loc=0, column=\"hour_offset\", value=(df.reset_index().index*2))\n",
    "    df['hour_offset'] = df.hour_offset.astype('timedelta64[h]')\n",
    "    df['pred_dtime'] = df['pred_dtime'] + df.hour.astype('timedelta64[h]')\n",
    "    df[['height','period','direction']]=df.apply(best_height, axis=1)\n",
    "    df[['hits_at']]=df.apply(estimate_hits_at, axis=1)\n",
    "    df['beach_id'] = 1\n",
    "    df.drop(['height1', 'period1', 'direction1', 'height2', 'period2', 'direction2', 'hour_offset',\n",
    "              'height3', 'period3', 'direction3','hour', 'pred_day', 'pred_hour'], axis=1, inplace=True) \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a262c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred_hour']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf86420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13148 entries, 0 to 13147\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   beach_id    13148 non-null  int64         \n",
      " 1   height      13148 non-null  float64       \n",
      " 2   period      13148 non-null  float64       \n",
      " 3   direction   13148 non-null  int64         \n",
      " 4   hits_at     13148 non-null  datetime64[ns]\n",
      " 5   pred_dtime  13148 non-null  datetime64[ns]\n",
      " 6   pred_hour   13148 non-null  int64         \n",
      "dtypes: datetime64[ns](2), float64(2), int64(3)\n",
      "memory usage: 719.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba163ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"swells-clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231fb912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        False\n",
       "1        False\n",
       "2        False\n",
       "3        False\n",
       "4        False\n",
       "         ...  \n",
       "13143    False\n",
       "13144    False\n",
       "13145    False\n",
       "13146    False\n",
       "13147    False\n",
       "Length: 13148, dtype: bool"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = df.duplicated(subset=['hits_at'])\n",
    "res"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc8daca7",
   "metadata": {},
   "source": [
    "df['height'] = pd.to_numeric(df['height'] , errors='coerce').astype(np.float64)\n",
    "df['period'] = pd.to_numeric(df['period'] , errors='coerce').astype(np.float64)\n",
    "df['direction'] = pd.to_numeric(df['direction'] , errors='coerce').astype(np.int64)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e53774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_dtime</th>\n",
       "      <th>height</th>\n",
       "      <th>period</th>\n",
       "      <th>direction</th>\n",
       "      <th>hits_at</th>\n",
       "      <th>beach_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>2.31</td>\n",
       "      <td>9.9</td>\n",
       "      <td>118</td>\n",
       "      <td>2022-08-21 02:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>2.23</td>\n",
       "      <td>9.8</td>\n",
       "      <td>118</td>\n",
       "      <td>2022-08-21 04:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>2.16</td>\n",
       "      <td>9.7</td>\n",
       "      <td>119</td>\n",
       "      <td>2022-08-21 06:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>9.7</td>\n",
       "      <td>119</td>\n",
       "      <td>2022-08-21 08:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>2.05</td>\n",
       "      <td>9.6</td>\n",
       "      <td>120</td>\n",
       "      <td>2022-08-21 10:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>1.63</td>\n",
       "      <td>11.6</td>\n",
       "      <td>138</td>\n",
       "      <td>2022-09-21 18:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>1.69</td>\n",
       "      <td>11.5</td>\n",
       "      <td>138</td>\n",
       "      <td>2022-09-21 20:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>1.78</td>\n",
       "      <td>11.3</td>\n",
       "      <td>141</td>\n",
       "      <td>2022-09-21 22:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>1.75</td>\n",
       "      <td>11.2</td>\n",
       "      <td>137</td>\n",
       "      <td>2022-09-22 00:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>2022-08-20 18:00:00</td>\n",
       "      <td>1.81</td>\n",
       "      <td>11.1</td>\n",
       "      <td>138</td>\n",
       "      <td>2022-09-22 02:00:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>385 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pred_dtime  height  period  direction             hits_at  \\\n",
       "0   2022-08-20 18:00:00    2.31     9.9        118 2022-08-21 02:00:00   \n",
       "1   2022-08-20 18:00:00    2.23     9.8        118 2022-08-21 04:00:00   \n",
       "2   2022-08-20 18:00:00    2.16     9.7        119 2022-08-21 06:00:00   \n",
       "3   2022-08-20 18:00:00    2.10     9.7        119 2022-08-21 08:00:00   \n",
       "4   2022-08-20 18:00:00    2.05     9.6        120 2022-08-21 10:00:00   \n",
       "..                  ...     ...     ...        ...                 ...   \n",
       "380 2022-08-20 18:00:00    1.63    11.6        138 2022-09-21 18:00:00   \n",
       "381 2022-08-20 18:00:00    1.69    11.5        138 2022-09-21 20:00:00   \n",
       "382 2022-08-20 18:00:00    1.78    11.3        141 2022-09-21 22:00:00   \n",
       "383 2022-08-20 18:00:00    1.75    11.2        137 2022-09-22 00:00:00   \n",
       "384 2022-08-20 18:00:00    1.81    11.1        138 2022-09-22 02:00:00   \n",
       "\n",
       "     beach_id  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "..        ...  \n",
       "380         1  \n",
       "381         1  \n",
       "382         1  \n",
       "383         1  \n",
       "384         1  \n",
       "\n",
       "[385 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3490c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches = [\"height\", \"period\", \"direction\", \"hits_at\"]\n",
    "\n",
    "# if BACKFILL == False:\n",
    "#     entry = []\n",
    "#     data = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         if (index==0):\n",
    "#             data.append(row['beach_id'])\n",
    "#             data.append(row['pred_dtime'])\n",
    "#         if (index < hours):\n",
    "#             for m in matches:\n",
    "#                 data.append(row[m])\n",
    "\n",
    "#     entry.append(data)\n",
    "#     first_columns=['beach_id', 'pred_dtime', 'height', 'period', 'direction', 'hits_at']    \n",
    "#     all_columns = first_columns + secondary_columns\n",
    "#     df2 = pd.DataFrame(entry, columns=all_columns)\n",
    "# else:    \n",
    "#     df2=df\n",
    "\n",
    "# for i in range(1,hours):\n",
    "#     for j in matches:\n",
    "#       df2[j+str(i*2)] = pd.to_numeric(df2[j+str(i*2)]).astype(np.float64)\n",
    "# df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b00b8a",
   "metadata": {},
   "source": [
    "### Connect to your Hopsworks cluster\n",
    "\n",
    "If you only set the HOPSWORKS_API_KEY, it will assume you are connecting to app.hopsworks.ai.\n",
    "Set HOPSWORKS_HOST and HOPSWORKS_PROJECT environment variables to connect to a different Hopsworks cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "671ea0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/398\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4686d8",
   "metadata": {},
   "source": [
    "Write your features to the `swells_exploded` feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bc6ed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc3205a3d9b44e39b0a0de34b282f53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/385 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching offline feature group backfill job...\n",
      "Backfill Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/398/jobs/named/swells_exploded_1_offline_fg_backfill/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7fa05818da30>, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swells_fg = fs.get_or_create_feature_group(name=\"swells_exploded\",\n",
    "                version=version,\n",
    "                primary_key=[\"beach_id\"],\n",
    "                event_time=\"hits_at\",\n",
    "                description=\"Buoy surf height predictions\",\n",
    "                statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True}\n",
    "                )\n",
    "swells_fg.insert(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fe8fac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
