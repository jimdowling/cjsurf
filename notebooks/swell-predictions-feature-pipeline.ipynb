{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee6d89da",
   "metadata": {},
   "source": [
    "## Feature Pipeline for the `exploded_swells` feature group\n",
    "\n",
    "This feature pipeline can be run on a schedule using github actions (see github repo for the example file).\n",
    "\n",
    "### Requirements\n",
    "\n",
    " * hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1939145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cdbcfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request  \n",
    "import re\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hopsworks\n",
    "from datetime import datetime, timedelta\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46146619",
   "metadata": {},
   "source": [
    "### Not app.hopsworks.ai ?\n",
    "\n",
    "If you are running your own Hopsworks cluster (not app.hopsworks.ai):\n",
    "\n",
    " * uncomment the cell below\n",
    " * fill in details for your cluster\n",
    " * run the cel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ca44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#key=\"\"\n",
    "#with open(\"api-key.txt\", \"r\") as f:\n",
    "#    key = f.read().rstrip()\n",
    "#os.environ['HOPSWORKS_PROJECT']=\"cjsurf\"\n",
    "#os.environ['HOPSWORKS_HOST']=\"35.187.178.84\"\n",
    "#os.environ['HOPSWORKS_API_KEY']=key    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c163f",
   "metadata": {},
   "source": [
    "### Backfill the feature group \n",
    "\n",
    "If you set `BACKFILL` to `True` in the cell below, and continue running all the cells, you will insert swell predictions from the `swells-clean.csv` file into the feature group.\n",
    "\n",
    "When `BACKFILL` is `False`, it will download the latest predictions from the NOA 62081 Buoy and insert them into the feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fe2886",
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKFILL=False\n",
    "if os.environ.get('BACKFILL') == \"False\":\n",
    "    BACKFILL=False\n",
    "hours=119\n",
    "version=1\n",
    "backfill_url=\"https://repo.hops.works/master/hopsworks-tutorials/data/cjsurf/swells-clean.csv\"\n",
    "buoy=\"62081\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2163acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_url(today):\n",
    "    pred_date = today.strftime(\"%Y%m%d\")\n",
    "\n",
    "    # There are 4 predictions per day at hours: \"00\", \"06\", \"12\", \"18\",\n",
    "    h=int(today.strftime(\"%H\"))\n",
    "    found = False\n",
    "    test_url = \"\"\n",
    "    attempted_date = pred_date\n",
    "    while not found:\n",
    "        pred_hour = \"00\"\n",
    "        if h > 5:\n",
    "            pred_hour = \"06\"\n",
    "        if h > 11:\n",
    "            pred_hour = \"12\" \n",
    "        if h > 17:\n",
    "            pred_hour = \"18\"\n",
    "        test_url = \"https://ftpprd.ncep.noaa.gov/data/nccf/com/gfs/prod/gfs.\" + attempted_date + \\\n",
    "      \"/\" + pred_hour + \"/wave/station/bulls.t\" + pred_hour + \"z/gfswave.\" + buoy + \".bull\"\n",
    "        try:\n",
    "            urllib.request.urlopen(test_url)\n",
    "            found = True\n",
    "        except urllib.error.HTTPError as e: \n",
    "            # assume 404, URL not found. Try previous time.\n",
    "            h = h - 6\n",
    "            if h < 0:\n",
    "                attempted_date = attempted_date - timedelta(days=days_to_subtract)\n",
    "                if (pred_date - attempted_date > 1):\n",
    "                    sys.exit(\"ERROR: Could not download url: \" + test_url) \n",
    "    print(test_url)\n",
    "    return test_url, pred_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9c9da",
   "metadata": {},
   "source": [
    "### Understand the Features\n",
    "\n",
    "We store 119*4=476 columns in the `swell_predictions` feature group. It is 119 different swell predictions, one for each hour from hour=0, hour=2, ..., hour=238.  Each prediction is made using the `height`, `period`, and `direction` features. The `hits_at` feature is used to estimate the time at which the swell arrives at Lahinch beach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fc73480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['height2',\n",
       " 'period2',\n",
       " 'direction2',\n",
       " 'hits_at2',\n",
       " 'height4',\n",
       " 'period4',\n",
       " 'direction4',\n",
       " 'hits_at4',\n",
       " 'height6',\n",
       " 'period6',\n",
       " 'direction6',\n",
       " 'hits_at6',\n",
       " 'height8',\n",
       " 'period8',\n",
       " 'direction8',\n",
       " 'hits_at8',\n",
       " 'height10',\n",
       " 'period10',\n",
       " 'direction10',\n",
       " 'hits_at10',\n",
       " 'height12',\n",
       " 'period12',\n",
       " 'direction12',\n",
       " 'hits_at12',\n",
       " 'height14',\n",
       " 'period14',\n",
       " 'direction14',\n",
       " 'hits_at14',\n",
       " 'height16',\n",
       " 'period16',\n",
       " 'direction16',\n",
       " 'hits_at16',\n",
       " 'height18',\n",
       " 'period18',\n",
       " 'direction18',\n",
       " 'hits_at18',\n",
       " 'height20',\n",
       " 'period20',\n",
       " 'direction20',\n",
       " 'hits_at20',\n",
       " 'height22',\n",
       " 'period22',\n",
       " 'direction22',\n",
       " 'hits_at22',\n",
       " 'height24',\n",
       " 'period24',\n",
       " 'direction24',\n",
       " 'hits_at24',\n",
       " 'height26',\n",
       " 'period26',\n",
       " 'direction26',\n",
       " 'hits_at26',\n",
       " 'height28',\n",
       " 'period28',\n",
       " 'direction28',\n",
       " 'hits_at28',\n",
       " 'height30',\n",
       " 'period30',\n",
       " 'direction30',\n",
       " 'hits_at30',\n",
       " 'height32',\n",
       " 'period32',\n",
       " 'direction32',\n",
       " 'hits_at32',\n",
       " 'height34',\n",
       " 'period34',\n",
       " 'direction34',\n",
       " 'hits_at34',\n",
       " 'height36',\n",
       " 'period36',\n",
       " 'direction36',\n",
       " 'hits_at36',\n",
       " 'height38',\n",
       " 'period38',\n",
       " 'direction38',\n",
       " 'hits_at38',\n",
       " 'height40',\n",
       " 'period40',\n",
       " 'direction40',\n",
       " 'hits_at40',\n",
       " 'height42',\n",
       " 'period42',\n",
       " 'direction42',\n",
       " 'hits_at42',\n",
       " 'height44',\n",
       " 'period44',\n",
       " 'direction44',\n",
       " 'hits_at44',\n",
       " 'height46',\n",
       " 'period46',\n",
       " 'direction46',\n",
       " 'hits_at46',\n",
       " 'height48',\n",
       " 'period48',\n",
       " 'direction48',\n",
       " 'hits_at48',\n",
       " 'height50',\n",
       " 'period50',\n",
       " 'direction50',\n",
       " 'hits_at50',\n",
       " 'height52',\n",
       " 'period52',\n",
       " 'direction52',\n",
       " 'hits_at52',\n",
       " 'height54',\n",
       " 'period54',\n",
       " 'direction54',\n",
       " 'hits_at54',\n",
       " 'height56',\n",
       " 'period56',\n",
       " 'direction56',\n",
       " 'hits_at56',\n",
       " 'height58',\n",
       " 'period58',\n",
       " 'direction58',\n",
       " 'hits_at58',\n",
       " 'height60',\n",
       " 'period60',\n",
       " 'direction60',\n",
       " 'hits_at60',\n",
       " 'height62',\n",
       " 'period62',\n",
       " 'direction62',\n",
       " 'hits_at62',\n",
       " 'height64',\n",
       " 'period64',\n",
       " 'direction64',\n",
       " 'hits_at64',\n",
       " 'height66',\n",
       " 'period66',\n",
       " 'direction66',\n",
       " 'hits_at66',\n",
       " 'height68',\n",
       " 'period68',\n",
       " 'direction68',\n",
       " 'hits_at68',\n",
       " 'height70',\n",
       " 'period70',\n",
       " 'direction70',\n",
       " 'hits_at70',\n",
       " 'height72',\n",
       " 'period72',\n",
       " 'direction72',\n",
       " 'hits_at72',\n",
       " 'height74',\n",
       " 'period74',\n",
       " 'direction74',\n",
       " 'hits_at74',\n",
       " 'height76',\n",
       " 'period76',\n",
       " 'direction76',\n",
       " 'hits_at76',\n",
       " 'height78',\n",
       " 'period78',\n",
       " 'direction78',\n",
       " 'hits_at78',\n",
       " 'height80',\n",
       " 'period80',\n",
       " 'direction80',\n",
       " 'hits_at80',\n",
       " 'height82',\n",
       " 'period82',\n",
       " 'direction82',\n",
       " 'hits_at82',\n",
       " 'height84',\n",
       " 'period84',\n",
       " 'direction84',\n",
       " 'hits_at84',\n",
       " 'height86',\n",
       " 'period86',\n",
       " 'direction86',\n",
       " 'hits_at86',\n",
       " 'height88',\n",
       " 'period88',\n",
       " 'direction88',\n",
       " 'hits_at88',\n",
       " 'height90',\n",
       " 'period90',\n",
       " 'direction90',\n",
       " 'hits_at90',\n",
       " 'height92',\n",
       " 'period92',\n",
       " 'direction92',\n",
       " 'hits_at92',\n",
       " 'height94',\n",
       " 'period94',\n",
       " 'direction94',\n",
       " 'hits_at94',\n",
       " 'height96',\n",
       " 'period96',\n",
       " 'direction96',\n",
       " 'hits_at96',\n",
       " 'height98',\n",
       " 'period98',\n",
       " 'direction98',\n",
       " 'hits_at98',\n",
       " 'height100',\n",
       " 'period100',\n",
       " 'direction100',\n",
       " 'hits_at100',\n",
       " 'height102',\n",
       " 'period102',\n",
       " 'direction102',\n",
       " 'hits_at102',\n",
       " 'height104',\n",
       " 'period104',\n",
       " 'direction104',\n",
       " 'hits_at104',\n",
       " 'height106',\n",
       " 'period106',\n",
       " 'direction106',\n",
       " 'hits_at106',\n",
       " 'height108',\n",
       " 'period108',\n",
       " 'direction108',\n",
       " 'hits_at108',\n",
       " 'height110',\n",
       " 'period110',\n",
       " 'direction110',\n",
       " 'hits_at110',\n",
       " 'height112',\n",
       " 'period112',\n",
       " 'direction112',\n",
       " 'hits_at112',\n",
       " 'height114',\n",
       " 'period114',\n",
       " 'direction114',\n",
       " 'hits_at114',\n",
       " 'height116',\n",
       " 'period116',\n",
       " 'direction116',\n",
       " 'hits_at116',\n",
       " 'height118',\n",
       " 'period118',\n",
       " 'direction118',\n",
       " 'hits_at118',\n",
       " 'height120',\n",
       " 'period120',\n",
       " 'direction120',\n",
       " 'hits_at120',\n",
       " 'height122',\n",
       " 'period122',\n",
       " 'direction122',\n",
       " 'hits_at122',\n",
       " 'height124',\n",
       " 'period124',\n",
       " 'direction124',\n",
       " 'hits_at124',\n",
       " 'height126',\n",
       " 'period126',\n",
       " 'direction126',\n",
       " 'hits_at126',\n",
       " 'height128',\n",
       " 'period128',\n",
       " 'direction128',\n",
       " 'hits_at128',\n",
       " 'height130',\n",
       " 'period130',\n",
       " 'direction130',\n",
       " 'hits_at130',\n",
       " 'height132',\n",
       " 'period132',\n",
       " 'direction132',\n",
       " 'hits_at132',\n",
       " 'height134',\n",
       " 'period134',\n",
       " 'direction134',\n",
       " 'hits_at134',\n",
       " 'height136',\n",
       " 'period136',\n",
       " 'direction136',\n",
       " 'hits_at136',\n",
       " 'height138',\n",
       " 'period138',\n",
       " 'direction138',\n",
       " 'hits_at138',\n",
       " 'height140',\n",
       " 'period140',\n",
       " 'direction140',\n",
       " 'hits_at140',\n",
       " 'height142',\n",
       " 'period142',\n",
       " 'direction142',\n",
       " 'hits_at142',\n",
       " 'height144',\n",
       " 'period144',\n",
       " 'direction144',\n",
       " 'hits_at144',\n",
       " 'height146',\n",
       " 'period146',\n",
       " 'direction146',\n",
       " 'hits_at146',\n",
       " 'height148',\n",
       " 'period148',\n",
       " 'direction148',\n",
       " 'hits_at148',\n",
       " 'height150',\n",
       " 'period150',\n",
       " 'direction150',\n",
       " 'hits_at150',\n",
       " 'height152',\n",
       " 'period152',\n",
       " 'direction152',\n",
       " 'hits_at152',\n",
       " 'height154',\n",
       " 'period154',\n",
       " 'direction154',\n",
       " 'hits_at154',\n",
       " 'height156',\n",
       " 'period156',\n",
       " 'direction156',\n",
       " 'hits_at156',\n",
       " 'height158',\n",
       " 'period158',\n",
       " 'direction158',\n",
       " 'hits_at158',\n",
       " 'height160',\n",
       " 'period160',\n",
       " 'direction160',\n",
       " 'hits_at160',\n",
       " 'height162',\n",
       " 'period162',\n",
       " 'direction162',\n",
       " 'hits_at162',\n",
       " 'height164',\n",
       " 'period164',\n",
       " 'direction164',\n",
       " 'hits_at164',\n",
       " 'height166',\n",
       " 'period166',\n",
       " 'direction166',\n",
       " 'hits_at166',\n",
       " 'height168',\n",
       " 'period168',\n",
       " 'direction168',\n",
       " 'hits_at168',\n",
       " 'height170',\n",
       " 'period170',\n",
       " 'direction170',\n",
       " 'hits_at170',\n",
       " 'height172',\n",
       " 'period172',\n",
       " 'direction172',\n",
       " 'hits_at172',\n",
       " 'height174',\n",
       " 'period174',\n",
       " 'direction174',\n",
       " 'hits_at174',\n",
       " 'height176',\n",
       " 'period176',\n",
       " 'direction176',\n",
       " 'hits_at176',\n",
       " 'height178',\n",
       " 'period178',\n",
       " 'direction178',\n",
       " 'hits_at178',\n",
       " 'height180',\n",
       " 'period180',\n",
       " 'direction180',\n",
       " 'hits_at180',\n",
       " 'height182',\n",
       " 'period182',\n",
       " 'direction182',\n",
       " 'hits_at182',\n",
       " 'height184',\n",
       " 'period184',\n",
       " 'direction184',\n",
       " 'hits_at184',\n",
       " 'height186',\n",
       " 'period186',\n",
       " 'direction186',\n",
       " 'hits_at186',\n",
       " 'height188',\n",
       " 'period188',\n",
       " 'direction188',\n",
       " 'hits_at188',\n",
       " 'height190',\n",
       " 'period190',\n",
       " 'direction190',\n",
       " 'hits_at190',\n",
       " 'height192',\n",
       " 'period192',\n",
       " 'direction192',\n",
       " 'hits_at192',\n",
       " 'height194',\n",
       " 'period194',\n",
       " 'direction194',\n",
       " 'hits_at194',\n",
       " 'height196',\n",
       " 'period196',\n",
       " 'direction196',\n",
       " 'hits_at196',\n",
       " 'height198',\n",
       " 'period198',\n",
       " 'direction198',\n",
       " 'hits_at198',\n",
       " 'height200',\n",
       " 'period200',\n",
       " 'direction200',\n",
       " 'hits_at200',\n",
       " 'height202',\n",
       " 'period202',\n",
       " 'direction202',\n",
       " 'hits_at202',\n",
       " 'height204',\n",
       " 'period204',\n",
       " 'direction204',\n",
       " 'hits_at204',\n",
       " 'height206',\n",
       " 'period206',\n",
       " 'direction206',\n",
       " 'hits_at206',\n",
       " 'height208',\n",
       " 'period208',\n",
       " 'direction208',\n",
       " 'hits_at208',\n",
       " 'height210',\n",
       " 'period210',\n",
       " 'direction210',\n",
       " 'hits_at210',\n",
       " 'height212',\n",
       " 'period212',\n",
       " 'direction212',\n",
       " 'hits_at212',\n",
       " 'height214',\n",
       " 'period214',\n",
       " 'direction214',\n",
       " 'hits_at214',\n",
       " 'height216',\n",
       " 'period216',\n",
       " 'direction216',\n",
       " 'hits_at216',\n",
       " 'height218',\n",
       " 'period218',\n",
       " 'direction218',\n",
       " 'hits_at218',\n",
       " 'height220',\n",
       " 'period220',\n",
       " 'direction220',\n",
       " 'hits_at220',\n",
       " 'height222',\n",
       " 'period222',\n",
       " 'direction222',\n",
       " 'hits_at222',\n",
       " 'height224',\n",
       " 'period224',\n",
       " 'direction224',\n",
       " 'hits_at224',\n",
       " 'height226',\n",
       " 'period226',\n",
       " 'direction226',\n",
       " 'hits_at226',\n",
       " 'height228',\n",
       " 'period228',\n",
       " 'direction228',\n",
       " 'hits_at228',\n",
       " 'height230',\n",
       " 'period230',\n",
       " 'direction230',\n",
       " 'hits_at230',\n",
       " 'height232',\n",
       " 'period232',\n",
       " 'direction232',\n",
       " 'hits_at232',\n",
       " 'height234',\n",
       " 'period234',\n",
       " 'direction234',\n",
       " 'hits_at234',\n",
       " 'height236',\n",
       " 'period236',\n",
       " 'direction236',\n",
       " 'hits_at236']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "secondary_columns=[]\n",
    "for i in range(1,hours):\n",
    "    j=i*2\n",
    "    secondary_columns.append(\"height\" + str(j))\n",
    "    secondary_columns.append(\"period\" + str(j))\n",
    "    secondary_columns.append(\"direction\" + str(j))\n",
    "    secondary_columns.append(\"hits_at\" + str(j))\n",
    "\n",
    "secondary_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751537c6",
   "metadata": {},
   "source": [
    "Parse the data in the URL managed by NOA containing the predictions for the Buoy:\n",
    "\n",
    "https://ftpprd.ncep.noaa.gov/data/nccf/com/gfs/prod/gfs.DATE/HOUR/wave/station/bulls.tHOURz/gfswave.62081.bull \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e410a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(buoy_url):\n",
    "    out = []\n",
    "    for line in urllib.request.urlopen(buoy_url):\n",
    "        l = line.decode('utf-8') #utf-8 or iso8859-1 or whatever the page encoding scheme is\n",
    "        row=[]\n",
    "        if \"Cycle\" in l:\n",
    "            regex = re.findall(r'Cycle.*:\\s+([0-9]+)\\s+([0-9]+)\\s+UTC.*', l)\n",
    "            if len(regex):\n",
    "                thedate=regex[0]\n",
    "        else:\n",
    "            res = re.match(r'.*[|]\\s+([0-9]+)\\s+([0-9]+)\\s+[|].*', l)\n",
    "            waves = re.findall(r'[|]\\s+([0-9\\.]+)\\s+([0-9\\.]+)\\s+([0-9]+)\\s+[|]', l)\n",
    "            if res is not None:\n",
    "                row.append(thedate)\n",
    "                row.append(res.groups())\n",
    "            if len(waves):\n",
    "                if len(waves) > 3:\n",
    "                    # print(\"found > 3 waves, reduce to 3\")\n",
    "                    waves = waves[:3]\n",
    "                b = []\n",
    "                list(b.extend(item) for item in waves)\n",
    "                row.append(b)\n",
    "                my = tuple(chain.from_iterable(row))\n",
    "                out.append(my)\n",
    "    return out, thedate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff4b79",
   "metadata": {},
   "source": [
    "### Feature engineering - select the best swell for Lahinch\n",
    "\n",
    "There are between zero and 6 different swells. Extract the swell that is gives the expected highest surf at Lahinch, based on the angle of the swell direction (Lahinch has a swell direction window of around 20 degrees to 120 degrees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0c54672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ftpprd.ncep.noaa.gov/data/nccf/com/gfs/prod/gfs.20220901/06/wave/station/bulls.t06z/gfswave.62081.bull\n",
      "https://ftpprd.ncep.noaa.gov/data/nccf/com/gfs/prod/gfs.20220901/06/wave/station/bulls.t06z/gfswave.62081.bull\n"
     ]
    }
   ],
   "source": [
    "primary_columns=['pred_dtime', 'hour', 'pred_day', 'pred_hour', 'height1', 'period1', 'direction1', 'height2', \n",
    "         'period2', 'direction2', 'height3', 'period3', 'direction3'] \n",
    "\n",
    "def is_valid_swell_direction(direction):\n",
    "    if int(direction) > 180 or int(direction) < 20:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def best_height(row):\n",
    "    best_secondary=2\n",
    "    # Check which is best secondary swell - swell 2 or swell 3?\n",
    "    if row['direction3'] != None:\n",
    "        if is_valid_swell_direction(row['direction3']):\n",
    "            if is_valid_swell_direction(row['direction2']) == False :\n",
    "                best_secondary=3    \n",
    "    best_direction = \"direction\" + str(best_secondary)\n",
    "    best=1\n",
    "    # Check which is best of swell 1 and secondary swell ?\n",
    "    if row[best_direction] != None and is_valid_swell_direction(row[best_direction]) == True:\n",
    "        if is_valid_swell_direction(row['direction1']) == False:\n",
    "            best=best_secondary\n",
    "                \n",
    "    height = row['height' + str(best)]\n",
    "    period = row['period' + str(best)]\n",
    "    direction = row['direction' + str(best)]\n",
    "        \n",
    "    return pd.Series([height, period, direction])\n",
    "\n",
    "# feature engineering - estimate the time at which the swell arrives at Lahinch from buoy\n",
    "def estimate_hits_at(row):\n",
    "    # baseline estimate\n",
    "    hits_at = row['pred_dtime'] + row['hour_offset'] + timedelta(hours=8) \n",
    "    \n",
    "    if float(row['direction']) < 80 and float(row['direction']) > 66:\n",
    "        hits_at = hits_at - timedelta(hours=1)\n",
    "    if float(row['direction']) <= 66 and float(row['direction']) > 50:\n",
    "        hits_at = hits_at - timedelta(hours=2)\n",
    "    if float(row['direction']) <= 50 and float(row['direction']) > 20:\n",
    "        hits_at = hits_at - timedelta(hours=3)\n",
    "    if float(row['period']) > 12:\n",
    "        hits_at = hits_at - timedelta(hours=1)\n",
    "    \n",
    "    return pd.Series([hits_at])\n",
    "    \n",
    "\n",
    "if BACKFILL == True:\n",
    "    df = pd.read_csv(backfill_url, parse_dates=['hits_at', 'pred_dtime'])\n",
    "    num_rows = df.shape[0]\n",
    "    print(\"num_rows: \" + str(num_rows))\n",
    "    rows = []\n",
    "    for i in range(1, num_rows):\n",
    "        row=[]\n",
    "        for j in range(0, len(secondary_columns)):\n",
    "            row.append(\"\")\n",
    "        if i % 2 == 0:\n",
    "            rows.append(row)\n",
    "    df_secondary = pd.DataFrame(rows, columns=secondary_columns)\n",
    "    df = pd.concat([df, df_secondary],axis=1, join=\"outer\")    \n",
    "    \n",
    "else: # BACKFILL == False\n",
    "    today = datetime.now()\n",
    "    url, pred_hour = get_latest_url(today)\n",
    "    print(url)\n",
    "    res,thedate=process_url(url)\n",
    "    df = pd.DataFrame(res, columns=primary_columns)\n",
    "    df['pred_dtime'] = pd.to_datetime(df['pred_dtime'], format='%Y%m%d')\n",
    "    df.insert(loc=0, column=\"hour_offset\", value=(df.reset_index().index*2))\n",
    "    df['hour_offset'] = df.hour_offset.astype('timedelta64[h]')\n",
    "    df['pred_dtime'] = df['pred_dtime'] + df.hour.astype('timedelta64[h]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7636c633",
   "metadata": {},
   "outputs": [],
   "source": [
    "if BACKFILL == False:\n",
    "    df[['height','period','direction']]=df.apply(best_height, axis=1)\n",
    "    df[['hits_at']]=df.apply(estimate_hits_at, axis=1)\n",
    "    df['beach_id'] = 1\n",
    "    df.drop(['height1', 'period1', 'direction1', 'height2', 'period2', 'direction2', 'hour_offset',\n",
    "              'height3', 'period3', 'direction3','hour', 'pred_day', 'pred_hour'], axis=1, inplace=True) \n",
    "\n",
    "df['height'] = pd.to_numeric(df['height'] , errors='coerce').astype(np.float64)\n",
    "df['period'] = pd.to_numeric(df['period'] , errors='coerce').astype(np.float64)\n",
    "df['direction'] = pd.to_numeric(df['direction'] , errors='coerce').astype(np.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3490c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beach_id</th>\n",
       "      <th>pred_dtime</th>\n",
       "      <th>height</th>\n",
       "      <th>period</th>\n",
       "      <th>direction</th>\n",
       "      <th>hits_at</th>\n",
       "      <th>height2</th>\n",
       "      <th>period2</th>\n",
       "      <th>direction2</th>\n",
       "      <th>hits_at2</th>\n",
       "      <th>...</th>\n",
       "      <th>direction232</th>\n",
       "      <th>hits_at232</th>\n",
       "      <th>height234</th>\n",
       "      <th>period234</th>\n",
       "      <th>direction234</th>\n",
       "      <th>hits_at234</th>\n",
       "      <th>height236</th>\n",
       "      <th>period236</th>\n",
       "      <th>direction236</th>\n",
       "      <th>hits_at236</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-09-01 06:00:00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>8.6</td>\n",
       "      <td>92</td>\n",
       "      <td>2022-09-01 14:00:00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>8.1</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.662048e+18</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.662876e+18</td>\n",
       "      <td>2.45</td>\n",
       "      <td>9.5</td>\n",
       "      <td>41.0</td>\n",
       "      <td>1.662872e+18</td>\n",
       "      <td>2.26</td>\n",
       "      <td>9.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>1.662880e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 478 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   beach_id          pred_dtime  height  period  direction  \\\n",
       "0         1 2022-09-01 06:00:00    0.47     8.6         92   \n",
       "\n",
       "              hits_at  height2  period2  direction2      hits_at2  ...  \\\n",
       "0 2022-09-01 14:00:00     0.49      8.1        94.0  1.662048e+18  ...   \n",
       "\n",
       "   direction232    hits_at232  height234  period234  direction234  \\\n",
       "0          15.0  1.662876e+18       2.45        9.5          41.0   \n",
       "\n",
       "     hits_at234  height236  period236  direction236    hits_at236  \n",
       "0  1.662872e+18       2.26        9.2          42.0  1.662880e+18  \n",
       "\n",
       "[1 rows x 478 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = [\"height\", \"period\", \"direction\", \"hits_at\"]\n",
    "\n",
    "if BACKFILL == False:\n",
    "    entry = []\n",
    "    data = []\n",
    "    for index, row in df.iterrows():\n",
    "        if (index==0):\n",
    "            data.append(row['beach_id'])\n",
    "            data.append(row['pred_dtime'])\n",
    "        if (index < hours):\n",
    "            for m in matches:\n",
    "                data.append(row[m])\n",
    "\n",
    "    entry.append(data)\n",
    "    first_columns=['beach_id', 'pred_dtime', 'height', 'period', 'direction', 'hits_at']    \n",
    "    all_columns = first_columns + secondary_columns\n",
    "    df2 = pd.DataFrame(entry, columns=all_columns)\n",
    "else:    \n",
    "    df2=df\n",
    "\n",
    "for i in range(1,hours):\n",
    "    for j in matches:\n",
    "      df2[j+str(i*2)] = pd.to_numeric(df2[j+str(i*2)]).astype(np.float64)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b00b8a",
   "metadata": {},
   "source": [
    "### Connect to your Hopsworks cluster\n",
    "\n",
    "If you only set the HOPSWORKS_API_KEY, it will assume you are connecting to app.hopsworks.ai.\n",
    "Set HOPSWORKS_HOST and HOPSWORKS_PROJECT environment variables to connect to a different Hopsworks cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "671ea0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/398\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4686d8",
   "metadata": {},
   "source": [
    "Write your features to the `swells_exploded` feature group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bc6ed56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dfe22ddf6bb43a5b50eee1edcd701eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/1 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching offline feature group backfill job...\n",
      "Backfill Job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/398/jobs/named/swells_exploded_1_offline_fg_backfill/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<hsfs.core.job.Job at 0x7fe8bde54a00>, None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swells_fg = fs.get_or_create_feature_group(name=\"swells_exploded\",\n",
    "                version=version,\n",
    "                primary_key=[\"beach_id\"],\n",
    "                event_time=\"hits_at\",\n",
    "                description=\"Buoy surf height predictions\",\n",
    "                online_enabled=True,\n",
    "                statistics_config={\"enabled\": True, \"histograms\": True, \"correlations\": True}\n",
    "                )\n",
    "swells_fg.insert(df2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ed797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
