{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d27febb",
   "metadata": {},
   "source": [
    "## Runs the batch prediction pipeline \n",
    "\n",
    "Writes predictions to `wave_predictions` feature group, creates a PNG with the surf height predictions, and uploads it to Hopsworks.\n",
    "\n",
    "### Requirements\n",
    "\n",
    " * hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be02aad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a01606",
   "metadata": {},
   "source": [
    "### Not app.hopsworks.ai ?\n",
    "\n",
    "If you are running your own Hopsworks cluster (not app.hopsworks.ai):\n",
    "\n",
    " * uncomment the cell below\n",
    " * fill in details for your cluster\n",
    " * run the cel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4b0347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this cell and fill in details if you are running your own Hopsworks cluster\n",
    "# key=\"\"\n",
    "# with open(\"api-key.txt\", \"r\") as f:\n",
    "#     key = f.read().rstrip()\n",
    "# os.environ['HOPSWORKS_PROJECT']=\"cjsurf\"\n",
    "# os.environ['HOPSWORKS_HOST']=\"35.187.178.84\"\n",
    "# os.environ['HOPSWORKS_API_KEY']=key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d840e9",
   "metadata": {},
   "source": [
    "### Connect to your Hopsworks cluster\n",
    "\n",
    "If you only set the HOPSWORKS_API_KEY, it will assume you are connecting to app.hopsworks.ai.\n",
    "Set HOPSWORKS_HOST and HOPSWORKS_PROJECT environment variables to connect to a different Hopsworks cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "838e5e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/398\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c675069",
   "metadata": {},
   "source": [
    "### Download the model from the model registry\n",
    "\n",
    "Then read the pickled file and unpickle the serialized object into the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c93edc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "mr = project.get_model_registry()\n",
    "model = mr.get_model(\"cjsurf_model\", version=1)\n",
    "model_dir = model.download()\n",
    "model = joblib.load(model_dir + \"/knn_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024407eb",
   "metadata": {},
   "source": [
    "### Get a reference to the feature view\n",
    "\n",
    "You need to initialize the feature view with a training data version if the feature view has transformations, as transformations need to know which training data statistics to use for normalization, encoding, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "525b4357",
   "metadata": {},
   "outputs": [],
   "source": [
    "fv = fs.get_feature_view(\"lahinch_surf\", version=1)\n",
    "#fv.init_serving(1)\n",
    "fv.init_batch_scoring(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad79fae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-21 17:19:49,691 INFO: USE `dowlingj_featurestore`\n",
      "2022-08-21 17:19:50,712 INFO: WITH right_fg0 AS (SELECT *\n",
      "FROM (SELECT `fg1`.`wave_height` `wave_height`, `fg1`.`beach_id` `join_pk_beach_id`, `fg1`.`observation_time` `join_evt_observation_time`, `fg0`.`pred_dtime` `pred_dtime`, `fg0`.`height` `height`, `fg0`.`period` `period`, `fg0`.`direction` `direction`, RANK() OVER (PARTITION BY `fg1`.`beach_id`, `fg1`.`observation_time` ORDER BY `fg0`.`hits_at` DESC) pit_rank_hopsworks\n",
      "FROM `dowlingj_featurestore`.`lahinch_1` `fg1`\n",
      "INNER JOIN `dowlingj_featurestore`.`swells_exploded_1` `fg0` ON `fg1`.`beach_id` = `fg0`.`beach_id` AND `fg1`.`observation_time` >= `fg0`.`hits_at`) NA\n",
      "WHERE `pit_rank_hopsworks` = 1) (SELECT `right_fg0`.`wave_height` `wave_height`, `right_fg0`.`pred_dtime` `pred_dtime`, `right_fg0`.`height` `height`, `right_fg0`.`period` `period`, `right_fg0`.`direction` `direction`\n",
      "FROM right_fg0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "VersionWarning: Incremented version to `2`.\n"
     ]
    }
   ],
   "source": [
    "# df = fv.training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7787f6",
   "metadata": {},
   "source": [
    "### Retreive the feature vector\n",
    "\n",
    "It comes back as a Python array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b735e6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20220821'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#key = {\"beach_id\" : 1}\n",
    "#vector = fv.get_feature_vector(key)\n",
    "#print(vector)\n",
    "\n",
    "today = datetime.now()\n",
    "pred_date = today.strftime(\"%Y%m%d\")\n",
    "pred_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06e1d0f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/398/featurestores/query). Server response: \nHTTP code: 422, HTTP reason: UNPROCESSABLE_ENTITY, error code: 120001, error msg: An argument was not provided or it was malformed., user msg: Invalid requested features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_750720/3353646240.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"20220818\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#  , end_time=pred_date)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/feature_view.py\u001b[0m in \u001b[0;36mget_batch_data\u001b[0;34m(self, start_time, end_time, read_options)\u001b[0m\n\u001b[1;32m    267\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_batch_scoring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         return self._feature_view_engine.get_batch_data(\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mget_batch_data\u001b[0;34m(self, feature_view_obj, start_time, end_time, training_dataset_version, transformation_functions, read_options)\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_group_accessibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         feature_dataframe = self.get_batch_query(\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         ).read(read_options=read_options)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/constructor/query.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, online, dataframe_type, read_options)\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0mdepending\u001b[0m \u001b[0mon\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0msql_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline_conn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         return engine.get_instance().sql(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/constructor/query.py\u001b[0m in \u001b[0;36m_prep_read\u001b[0;34m(self, online, read_options)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prep_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query_constructor_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0monline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/core/query_constructor_api.py\u001b[0m in \u001b[0;36mconstruct_query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         return fs_query.FsQuery.from_response_json(\n\u001b[0;32m---> 27\u001b[0;31m             _client._send_request(\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/decorators.py\u001b[0m in \u001b[0;36mif_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mNoHopsworksConnectionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mif_connected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hsfs/client/base.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRestAPIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/398/featurestores/query). Server response: \nHTTP code: 422, HTTP reason: UNPROCESSABLE_ENTITY, error code: 120001, error msg: An argument was not provided or it was malformed., user msg: Invalid requested features"
     ]
    }
   ],
   "source": [
    "\n",
    "df = fv.get_batch_data(start_time=\"20220818\") #  , end_time=pred_date)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e61630",
   "metadata": {},
   "source": [
    "### Refactor the feature vector\n",
    "\n",
    "We are going to make 119 predictions with the vector for hour=0, hour=2, .., hour=338.\n",
    "\n",
    "Each prediction for hour=X is made using a feature vector containing features from the swell:\n",
    "\n",
    " * height\n",
    " * period\n",
    " * direction\n",
    " \n",
    "Here, we have to extract the 119 feature vectors from the Python array returned from the feature store.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bb0811",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = []\n",
    "# Remove the 'pred_dtime' column\n",
    "feature_vectors.append(vector[0:3])\n",
    "for i in range(4,len(vector),4):\n",
    "    feature_vectors.append(vector[i:i+3])\n",
    "dt = vector[3] # 'pred_dtime'\n",
    "feature_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f549c595",
   "metadata": {},
   "source": [
    "### Make  surf height predictions\n",
    "\n",
    "119 predictions for hour=0, hour=2, ..., hour=338.\n",
    "\n",
    "Store the predictions in `row_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e198c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_list=[]\n",
    "columns=[\"beach_id\",\"hits_at\", \"wave_height\"]\n",
    "for i in range(0,len(feature_vectors)):\n",
    "    arr=[]\n",
    "    arr.append(feature_vectors[i])\n",
    "    res = model.predict(arr)\n",
    "    row = []\n",
    "    row.append(1) #beach_id\n",
    "    hour = datetime.timedelta(0, (1*3600*i*2))\n",
    "    ts = dt + hour\n",
    "    dt_str = ts.strftime(\"%Y-%m-%d %H:%M\") #:%S\n",
    "    row.append(dt_str)\n",
    "    row.append(res[0])\n",
    "    row_list.append(row)\n",
    "\n",
    "df = pd.DataFrame(row_list, columns=columns)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6571e77a",
   "metadata": {},
   "source": [
    "### Insert the predictions into the feature group\n",
    "\n",
    "Store the predictions for later analysis, feature monitoring, model performance analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a61ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = project.get_feature_store()\n",
    "\n",
    "preds_fg = fs.get_or_create_feature_group(name=\"wave_predictions\",\n",
    "                version=1,\n",
    "                primary_key=[\"beach_id\", \"hits_at\"],\n",
    "                description=\"Lahinch surf height predictions\",\n",
    "                online_enabled=True\n",
    "                )\n",
    "preds_fg.insert(df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b9aaf",
   "metadata": {},
   "source": [
    "### Create PNG file with the surf height predictions\n",
    "\n",
    "Use plotly to create a chart with the surf height predictions. Upload it to the Resources directory in your project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ab0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.line(df, x = \"hits_at\", y = \"wave_height\", \n",
    "              #markers=True, \n",
    "              title = \"Wave Heights at Lahinch\"\n",
    "             )\n",
    "fig.update_layout(\n",
    "#    plot_bgcolor=\"white\",\n",
    "    margin=dict(t=50,l=10,b=10,r=10)\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis_tickformat = '%d/%m (%a)<br>Time %h:%m <br> %Y'\n",
    ")\n",
    "\n",
    "fig.update_layout()\n",
    "\n",
    "fig.update_xaxes(\n",
    "        ticks=\"outside\", \n",
    "        tickwidth=2,\n",
    "        tickcolor='black',\n",
    "        ticklen=10,\n",
    "        title_text = \"Time\",\n",
    "        title_font = {\"size\": 36},\n",
    "        title_standoff = 25)\n",
    "fig.update_yaxes(\n",
    "        title_text = \"Wave Height (ft)\",\n",
    "        title_font = {\"size\": 36},\n",
    "        title_standoff = 25,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "        font=dict(\n",
    "          family=\"Time\",\n",
    "          size=24,\n",
    "          color=\"black\"\n",
    "        )\n",
    ")\n",
    "\n",
    "filename=\"../latest_lahinch.png\"\n",
    "fig.write_image(file=filename, format=\"png\", width=1920, height=1280)\n",
    "dataset_api = project.get_dataset_api()\n",
    "uploaded_file_path = dataset_api.upload(filename, \"Resources\", overwrite=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: update images with predictions of previous week and outcomes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
